{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "This notebook is similar in functionality to [this python script](https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/mnist_series/mnist_tflearn.py), and is used with [this README](https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/mnist_series/02_README_mnist_tflearn.md).  It shows how to use TensorFlow's high-level Estimator classes to easily build a classifier with multiple hidden layers.\n",
    "\n",
    "First, do some imports and set some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# comment out for less info during the training runs.\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Set locations of data files\n",
    "MNIST_DATA_DIR = \"/tmp/MNIST_data\"\n",
    "FASHION_DATA_DIR = \"/tmp/fashion-mnist\"\n",
    "# Select your choice of dataset\n",
    "DATA_DIR = MNIST_DATA_DIR\n",
    "\n",
    "# read in data, downloading first as necessary\n",
    "DATA_SETS = input_data.read_data_sets(DATA_DIR)\n",
    "\n",
    "# define a utility function for generating a new directory in which to save \n",
    "# model information, so multiple training runs don't stomp on each other.\n",
    "def get_new_path(name=\"\"):\n",
    "    base=\"/tmp/tfmodels/mnist_estimators\"\n",
    "    logpath = os.path.join(base, name + \"_\" + str(int(time.time())))\n",
    "    print(\"Logging to {}\".format(logpath))\n",
    "    return logpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create an input function, using the `tf.train.shuffle_batch` function to take care of the batching and shuffling of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 40\n",
    "# call with generate_input_fn(DATA_SETS.train) or generate_input_fn(DATA_SETS.test)\n",
    "\n",
    "# These default settings will generate samples in the order of the file, forever.\n",
    "def generate_input_fn(dataset, batch_size=BATCH_SIZE, shuffle=False, epochs=None):\n",
    "    X = dataset.images\n",
    "    Y = dataset.labels.astype(numpy.int64)\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'pixels': X},\n",
    "        y=Y,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=epochs,\n",
    "        shuffle=shuffle\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first define a function that adds a LinearClassifier and runs its `train()` method, which will train the model. Note that we didn't need to explicitly define a model graph or a training loop ourselves.  \n",
    "\n",
    "Once we've trained the model, we run the `evaluate()` method, which uses the trained model. To do this, it loads the most recent checkpointed model info available.  The model checkpoint(s) will be generated during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_and_run_linear_classifier(num_steps, logdir, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Run a linear classifier.\"\"\"\n",
    "\n",
    "    feature_columns = [tf.feature_column.numeric_column(\n",
    "        \"pixels\", shape=784)]\n",
    "    \n",
    "    classifier = tf.estimator.LinearClassifier(\n",
    "                feature_columns=feature_columns, \n",
    "                n_classes=10,\n",
    "                model_dir=logdir\n",
    "                )\n",
    "    classifier.train(input_fn=generate_input_fn(DATA_SETS.train, \n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True), \n",
    "                     steps=num_steps)\n",
    "    \n",
    "    print(\"Finished training.\")\n",
    "    \n",
    "    # Evaluate accuracy.\n",
    "    accuracy_score = classifier.evaluate(input_fn=generate_input_fn(\n",
    "        DATA_SETS.test, batch_size, shuffle=False, epochs=1))['accuracy']\n",
    "    \n",
    "    print('Linear Classifier Accuracy: {0:f}'.format(accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add a function that defines a `DNNClassifier`, and runs its `train()` method, which will train the model. Again note that we didn't need to explicitly define a model graph or a training loop ourselves.  \n",
    "\n",
    "Then after we've trained the model, we run the classifier's `evaluate()` method, which uses the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_and_run_dnn_classifier(num_steps, logdir, lr=.1, batch_size=40):\n",
    "    \"\"\"Run a DNN classifier.\"\"\"\n",
    "    feature_columns = [tf.feature_column.numeric_column(\n",
    "        \"pixels\", shape=784)]\n",
    "    \n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns, \n",
    "        n_classes=10,\n",
    "        hidden_units=[200, 100, 50],\n",
    "        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=lr),\n",
    "        model_dir=logdir\n",
    "        )\n",
    "    # After you've done a training run with optimizer learning rate 0.1,\n",
    "        # change it to 0.5 and run the training again.  Use TensorBoard to take\n",
    "        # a look at the difference.  You can see both runs by pointing it to the\n",
    "        # parent model directory, which by default is:\n",
    "        #\n",
    "        #   tensorboard --logdir=/tmp/tfmodels/mnist_estimators\n",
    "        \n",
    "    classifier.train(input_fn=generate_input_fn(DATA_SETS.train, \n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True), \n",
    "                     steps=num_steps)\n",
    "\n",
    "    print(\"Finished running the deep training via the train() method\")\n",
    "    \n",
    "    accuracy_score = classifier.evaluate(input_fn=generate_input_fn(\n",
    "        DATA_SETS.test, batch_size=batch_size, shuffle=False, epochs=1))['accuracy']\n",
    "\n",
    "    print('DNN Classifier Accuracy: {0:f}'.format(accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the functions that define and train our classifiers. (It takes a moment to set up the input data queue before the training starts).\n",
    "\n",
    "Let's start with the LinearClassifier, which won't be very accurate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Linear classifier ...\n",
      "Logging to /tmp/tfmodels/mnist_estimators/linear_1505717722\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_model_dir': '/tmp/tfmodels/mnist_estimators/linear_1505717722', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tfmodels/mnist_estimators/linear_1505717722/model.ckpt.\n",
      "INFO:tensorflow:loss = 92.1034, step = 1\n",
      "INFO:tensorflow:global_step/sec: 868.742\n",
      "INFO:tensorflow:loss = 18.2605, step = 101 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 698.48\n",
      "INFO:tensorflow:loss = 13.0562, step = 201 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 612.347\n",
      "INFO:tensorflow:loss = 8.91486, step = 301 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 621.018\n",
      "INFO:tensorflow:loss = 13.0992, step = 401 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 576.715\n",
      "INFO:tensorflow:loss = 19.5502, step = 501 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 614.5\n",
      "INFO:tensorflow:loss = 8.06209, step = 601 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.984\n",
      "INFO:tensorflow:loss = 19.6814, step = 701 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 649.208\n",
      "INFO:tensorflow:loss = 13.0362, step = 801 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.779\n",
      "INFO:tensorflow:loss = 6.95756, step = 901 (0.156 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tfmodels/mnist_estimators/linear_1505717722/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.63498.\n",
      "Finished training.\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-18-06:55:27\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tfmodels/mnist_estimators/linear_1505717722/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-18-06:55:27\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9125, average_loss = 0.306952, global_step = 1000, loss = 12.2781\n",
      "Linear Classifier Accuracy: 0.912500\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Linear classifier ...\")\n",
    "define_and_run_linear_classifier(num_steps=1000, \n",
    "                                 logdir=get_new_path(\"linear\"), \n",
    "                                 batch_size=40)\n",
    "# With 1000 steps and a batch size of 40, we see accuracy of approx 91% for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the DNN Classifier.  First, let's try it with a .1 learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DNN classifier with .1 learning rate...\n",
      "Logging to /tmp/tfmodels/mnist_estimators/deep01_1505717746\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_model_dir': '/tmp/tfmodels/mnist_estimators/deep01_1505717746', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tfmodels/mnist_estimators/deep01_1505717746/model.ckpt.\n",
      "INFO:tensorflow:loss = 92.8653, step = 1\n",
      "INFO:tensorflow:global_step/sec: 474.408\n",
      "INFO:tensorflow:loss = 22.702, step = 101 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 354.874\n",
      "INFO:tensorflow:loss = 15.33, step = 201 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.932\n",
      "INFO:tensorflow:loss = 24.6402, step = 301 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 322.672\n",
      "INFO:tensorflow:loss = 11.3109, step = 401 (0.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.839\n",
      "INFO:tensorflow:loss = 7.75036, step = 501 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 318.28\n",
      "INFO:tensorflow:loss = 7.17263, step = 601 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 317.675\n",
      "INFO:tensorflow:loss = 10.8478, step = 701 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 464.408\n",
      "INFO:tensorflow:loss = 2.59377, step = 801 (0.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 320.224\n",
      "INFO:tensorflow:loss = 8.30866, step = 901 (0.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 338.595\n",
      "INFO:tensorflow:loss = 8.45203, step = 1001 (0.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 340.979\n",
      "INFO:tensorflow:loss = 7.0675, step = 1101 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 331.211\n",
      "INFO:tensorflow:loss = 18.3088, step = 1201 (0.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 322.09\n",
      "INFO:tensorflow:loss = 15.7891, step = 1301 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.784\n",
      "INFO:tensorflow:loss = 9.09692, step = 1401 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 351.038\n",
      "INFO:tensorflow:loss = 13.9041, step = 1501 (0.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 322.126\n",
      "INFO:tensorflow:loss = 2.10604, step = 1601 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.895\n",
      "INFO:tensorflow:loss = 8.32904, step = 1701 (0.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 333.346\n",
      "INFO:tensorflow:loss = 6.42847, step = 1801 (0.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.485\n",
      "INFO:tensorflow:loss = 2.6989, step = 1901 (0.335 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/tfmodels/mnist_estimators/deep01_1505717746/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.57996.\n",
      "Finished running the deep training via the train() method\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-18-06:55:57\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tfmodels/mnist_estimators/deep01_1505717746/model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-18-06:55:59\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9514, average_loss = 0.165727, global_step = 2000, loss = 6.62909\n",
      "DNN Classifier Accuracy: 0.951400\n"
     ]
    }
   ],
   "source": [
    "print(\"Running DNN classifier with .1 learning rate...\")\n",
    "classifier = define_and_run_dnn_classifier(2000, \n",
    "                                           get_new_path(\"deep01\"), \n",
    "                                           lr=.1)\n",
    "# With 2000 steps and a batch size of 40, we see accuracy of approx 95% on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's loop through the training-eval loop a couple of times, so we get more accuracy readings. Make a for-loop and provide a stable path for your model, which will allow continuous training-eval loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/tmp/tfmodels/mnist_estimators/deep01_1505716594\" # This is an example\n",
    "for i in range(0,5):\n",
    "    define_and_run_dnn_classifier(1000, model_path, lr=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what MNIST and fashion-mnist look like side by side. Change the path of the DATA_DIR to point to your fashion-mnist dataset, and run the training again. Be sure to change your model path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run it with a .5 learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Running DNN classifier with .5 learning rate...\")\n",
    "classifier = define_and_run_dnn_classifier(2000, \n",
    "                                           get_new_path(\"deep05\"), \n",
    "                                           lr=.5)\n",
    "# With 2000 steps and a batch size of 40, we see accuracy of approx 91%, though sometimes it does not converge at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare your results, start up TensorBoard as follows in a new terminal window. (If you get a 'not found' error, make sure you've activated your virtual environment in that new window):\n",
    "\n",
    "```sh\n",
    "$ tensorboard --logdir=/tmp/tfmodels/mnist_estimators\n",
    "```\n",
    "Or run the following (select Kernel --> Interrupt from the menu when you're done):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=/tmp/tfmodels/mnist_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
